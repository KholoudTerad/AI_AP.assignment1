# AI_AP.assignment1

Part 1: 

1. Explain the objective of the “transforms” function and explain the input and output for each line in the function. 
The "transforms" function serves the purpose of preprocessing the input data before it is fed into the model for training 
or evaluation.
a. Input: images and their corresponding captions.
Output: The function will return a dictionary containing the preprocessed inputs for the model.
def transforms(example_batch) :
b. Extract images from the example_batch dictionary:
Input: "example_batch["image"]" a list containing image data.
Output: "images" is a list containing the image data from example_batch
images = [x for x in example_batch["image"]]
c. Extract captions from the example_batch dictionary:
Input: "example_batch["text"]" a list containing text data (captions).
Output: "captions" a list containing the caption data from the example_batch.
captions = [x for x in example_batch["text"]] 
d. Process the images and captions using the specified processor :
Input: "images" and "captions" lists
Output: "inputs" a dictionary containing the preprocessed inputs for the model. "padding" is set to
"max_length", which means the sequences will be padded to the maximum length encountered in the batch.
inputs = processor(images=images, text=captions, padding="max_length")
e. Prepare the labels for the model by setting them equal to the input token IDs:
Input: "input_ids" key, containing the preprocessed inputs.
Output: "inputs" dictionary is updated to include the labels, which are set to be equal to the input token IDs.
inputs.update({"labels": inputs["input_ids"]})
f. Return the processed inputs:
Output: The function returns a dictionary containing the preprocessed inputs, including images, captions, and 
labe
return inputs
2. Explain the objective of “compute_metrics” and explain the input and output for each line in the function. 
a. Input: eval_pred is assumed to be a tuple containing two elements - logits and labels, representing the model 
predictions and the corresponding ground truth labels:
def compute_metrics(eval_pred) :
b. Unpack the evaluation predictions into logits and labels:
Input: eval_pred , containes model predictions (logits) and ground truth labels (labels) .
Output: logits and labels are variables containing the model's predictions and ground truth labels.
 logits, labels = eval_pred
 
c. Get the predicted labels by selecting the index with the highest logit value
Input: logits . Output: predicted
predicted = logits.argmax )1-(
 
d. Decode the labels and predictions using the processor's batch_decode method
Input: be tokenized sequences of labels and predicted tensors
Output: decoded_labels and decoded_predictions
 decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)
 decoded_predictions = processor.batch_decode(predicted, skip_special_tokens=True)
 
e. Compute the Word Error Rate (WER) using the wer.compute function
Input: decoded_predictions and decoded_labels lists of human-readable text .
Output: wer_score
wer_score = wer.compute(predictions=decoded_predictions, references=decoded_labels)
 
f. Return a dictionary containing the computed WER score:
Output: computed Word Error Rate.
return {"wer_score": wer_score}
3. What is the caption for the provided image in the link? " a man in a pirate costume"
4. Change the temperature to the following values and write the caption generated by each: 0.0 and 1.0
0.0: ValueError,`temperature` (=0.0) has to be a strictly positive float . unless if I changed do_sample to "False" 
then the output will be " a man in a suit and tie".
1.0: " a man sitting in a chair".


Part 2:

1. Write the generated captions for the same image as in Part 1.5: " a cartoon character".
2. Provide a concise analysis of the output observations, drawing comparisons between the captions identified in 
this context and the captions generated during training on the complete training dataset:
Comparing the captions identified during training on the limited training dataset (first 400 samples) " a cartoon 
character" with the captions generated during training on the complete training dataset " a man in a pirate costume" 
on the provided image, can reveal that the caption generated during training on a limited dataset lacks the variety 
and richness present in caption generated from the complete training dataset


Part 3:

1. Write the generated captions for the same image as in Part 1.5: " a girl in a blue costume".
2. Provide a concise analysis of the output observations, drawing comparisons between the captions identified in 
this context and the captions generated during training on the complete training dataset:
Comparing the captions with the filtering mechanism "a girl in a blue costume" with the captions without the filtering 
mechanism the filtering mechanism "a man in a pirate costume" on the provided image, can reveal that by excluding 
instances with words "man", "shirt", or "tie", the training process focuses more on learning captions related to other 
aspects of the images, like the gender based on the hair and color of clothing
